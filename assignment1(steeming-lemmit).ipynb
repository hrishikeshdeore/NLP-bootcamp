{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The corpus is taken from https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=\"\"\" class=\"mw-heading mw-heading3\"><h3 id=\"Symbolic_NLP_(1950s_–_early_1990s)\"><span id=\"Symbolic_NLP_.281950s_.E2.80.93_early_1990s.29\"></span>Symbolic NLP (1950s – early 1990s)</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Natural_language_processing&amp;action=edit&amp;section=2\" title=\"Edit section: Symbolic NLP (1950s – early 1990s)\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
    "<p>The premise of symbolic NLP is well-summarized by <a href=\"/wiki/John_Searle\" title=\"John Searle\">John Searle</a>'s <a href=\"/wiki/Chinese_room\" title=\"Chinese room\">Chinese room</a> experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n",
    "</p>\n",
    "<ul><li><b>1950s</b>: The <a href=\"/wiki/Georgetown-IBM_experiment\" class=\"mw-redirect\" title=\"Georgetown-IBM experiment\">Georgetown experiment</a> in 1954 involved fully <a href=\"/wiki/Automatic_translation\" class=\"mw-redirect\" title=\"Automatic translation\">automatic translation</a> of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\"><span class=\"cite-bracket\">&#91;</span>2<span class=\"cite-bracket\">&#93;</span></a></sup>  However, real progress was much slower, and after the <a href=\"/wiki/ALPAC\" title=\"ALPAC\">ALPAC report</a> in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\"><span class=\"cite-bracket\">&#91;</span>3<span class=\"cite-bracket\">&#93;</span></a></sup>) until the late 1980s when the first <a href=\"/wiki/Statistical_machine_translation\" title=\"Statistical machine translation\">statistical machine translation</a> systems were developed.</li>\n",
    "<li><b>1960s</b>: Some notably successful natural language processing systems developed in the 1960s were <a href=\"/wiki/SHRDLU\" title=\"SHRDLU\">SHRDLU</a>, a natural language system working in restricted \"<a href=\"/wiki/Blocks_world\" title=\"Blocks world\">blocks worlds</a>\" with restricted vocabularies, and <a href=\"/wiki/ELIZA\" title=\"ELIZA\">ELIZA</a>, a simulation of a <a href=\"/wiki/Rogerian_psychotherapy\" class=\"mw-redirect\" title=\"Rogerian psychotherapy\">Rogerian psychotherapist</a>, written by <a href=\"/wiki/Joseph_Weizenbaum\" title=\"Joseph Weizenbaum\">Joseph Weizenbaum</a> between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". <a href=\"/w/index.php?title=Ross_Quillian&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Ross Quillian (page does not exist)\">Ross Quillian</a>'s successful work on natural language was demonstrated with a vocabulary of only <i>twenty</i> words, because that was all that would fit in a computer  memory at the time.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\"><span class=\"cite-bracket\">&#91;</span>4<span class=\"cite-bracket\">&#93;</span></a></sup></li></ul>\n",
    "<ul><li><b>1970s</b>: During the 1970s, many programmers began to write \"conceptual <a href=\"/wiki/Ontology_(information_science)\" title=\"Ontology (information science)\">ontologies</a>\", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the first <a href=\"/wiki/Chatterbots\" class=\"mw-redirect\" title=\"Chatterbots\">chatterbots</a> were written (e.g., <a href=\"/wiki/PARRY\" title=\"PARRY\">PARRY</a>).</li>\n",
    "<li><b>1980s</b>: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of <a href=\"/wiki/Head-driven_phrase_structure_grammar\" title=\"Head-driven phrase structure grammar\">HPSG</a> as a computational operationalization of <a href=\"/wiki/Generative_grammar\" title=\"Generative grammar\">generative grammar</a>), morphology (e.g., two-level morphology<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\"><span class=\"cite-bracket\">&#91;</span>5<span class=\"cite-bracket\">&#93;</span></a></sup>), semantics (e.g., <a href=\"/wiki/Lesk_algorithm\" title=\"Lesk algorithm\">Lesk algorithm</a>), reference (e.g., within Centering Theory<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\"><span class=\"cite-bracket\">&#91;</span>6<span class=\"cite-bracket\">&#93;</span></a></sup>) and other areas of natural language understanding (e.g., in the <a href=\"/wiki/Rhetorical_structure_theory\" title=\"Rhetorical structure theory\">Rhetorical Structure Theory</a>). Other lines of research were continued, e.g., the development of chatterbots with <a href=\"/wiki/Racter\" title=\"Racter\">Racter</a> and <a href=\"/wiki/Jabberwacky\" title=\"Jabberwacky\">Jabberwacky</a>. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.<sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\"><span class=\"cite-bracket\">&#91;</span>7<span class=\"cite-bracket\">&#93;</span></a></sup></li></ul>\n",
    "<div class=\"mw-heading mw-heading3\"><h3 id=\"Statistical_NLP_(1990s–2010s)\"><span id=\"Statistical_NLP_.281990s.E2.80.932010s.29\"></span>Statistical NLP (1990s–2010s)</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Natural_language_processing&amp;action=edit&amp;section=3\" title=\"Edit section: Statistical NLP (1990s–2010s)\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
    "<p>Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a> algorithms for language processing.  This was due to both the steady increase in computational power (see <a href=\"/wiki/Moore%27s_law\" title=\"Moore&#39;s law\">Moore's law</a>) and the gradual lessening of the dominance of <a href=\"/wiki/Noam_Chomsky\" title=\"Noam Chomsky\">Chomskyan</a> theories of linguistics (e.g. <a href=\"/wiki/Transformational_grammar\" title=\"Transformational grammar\">transformational grammar</a>), whose theoretical underpinnings discouraged the sort of <a href=\"/wiki/Corpus_linguistics\" title=\"Corpus linguistics\">corpus linguistics</a> that underlies the machine-learning approach to language processing.<sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\"><span class=\"cite-bracket\">&#91;</span>8<span class=\"cite-bracket\">&#93;</span></a></sup> \n",
    "</p>\n",
    "<ul><li><b>1990s</b>: Many of the notable early successes in statistical methods in NLP occurred in the field of <a href=\"/wiki/Machine_translation\" title=\"Machine translation\">machine translation</a>, due especially to work at IBM Research, such as <a href=\"/wiki/IBM_alignment_models\" title=\"IBM alignment models\">IBM alignment models</a>.  These systems were able to take advantage of existing multilingual <a href=\"/wiki/Text_corpus\" title=\"Text corpus\">textual corpora</a> that had been produced by the <a href=\"/wiki/Parliament_of_Canada\" title=\"Parliament of Canada\">Parliament of Canada</a> and the <a href=\"/wiki/European_Union\" title=\"European Union\">European Union</a> as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.</li>\n",
    "<li><b>2000s</b>: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on <a href=\"/wiki/Unsupervised_learning\" title=\"Unsupervised learning\">unsupervised</a> and <a href=\"/wiki/Semi-supervised_learning\" class=\"mw-redirect\" title=\"Semi-supervised learning\">semi-supervised learning</a> algorithms.  Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult than <a href=\"/wiki/Supervised_learning\" title=\"Supervised learning\">supervised learning</a>, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the <a href=\"/wiki/World_Wide_Web\" title=\"World Wide Web\">World Wide Web</a>), which can often make up for the inferior results if the algorithm used has a low enough <a href=\"/wiki/Time_complexity\" title=\"Time complexity\">time complexity</a> to be practical.</li></ul>\n",
    "<div class=\"mw-heading mw-heading3\"><h3 id=\"Neural_NLP_(present)\"><span id=\"Neural_NLP_.28present.29\"></span>Neural NLP (present)</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Natural_language_processing&amp;action=edit&amp;section=4\" title=\"Edit section: Neural NLP (present)\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
    "<p>In 2003, <a href=\"/wiki/Word_n-gram_language_model\" title=\"Word n-gram language model\">word n-gram model</a>, at the time the best statistical algorithm, was outperformed by a <a href=\"/wiki/Multi-layer_perceptron\" class=\"mw-redirect\" title=\"Multi-layer perceptron\">multi-layer perceptron</a> (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in <a href=\"/wiki/Language_model\" title=\"Language model\">language modelling</a>) by <a href=\"/wiki/Yoshua_Bengio\" title=\"Yoshua Bengio\">Yoshua Bengio</a> with co-authors.<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\"><span class=\"cite-bracket\">&#91;</span>9<span class=\"cite-bracket\">&#93;</span></a></sup> \n",
    "</p><p>In 2010, <a href=\"/wiki/Tom%C3%A1%C5%A1_Mikolov\" title=\"Tomáš Mikolov\">Tomáš Mikolov</a> (then a PhD student at <a href=\"/wiki/Brno_University_of_Technology\" title=\"Brno University of Technology\">Brno University of Technology</a>) with co-authors applied a simple <a href=\"/wiki/Recurrent_neural_network\" title=\"Recurrent neural network\">recurrent neural network</a> with a single hidden layer to language modelling,<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\"><span class=\"cite-bracket\">&#91;</span>10<span class=\"cite-bracket\">&#93;</span></a></sup> and in the following years he went on to develop <a href=\"/wiki/Word2vec\" title=\"Word2vec\">Word2vec</a>. In the 2010s, <a href=\"/wiki/Representation_learning\" class=\"mw-redirect\" title=\"Representation learning\">representation learning</a> and <a href=\"/wiki/Deep_learning\" title=\"Deep learning\">deep neural network</a>-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques<sup id=\"cite_ref-goldberg:nnlp17_11-0\" class=\"reference\"><a href=\"#cite_note-goldberg:nnlp17-11\"><span class=\"cite-bracket\">&#91;</span>11<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-goodfellow:book16_12-0\" class=\"reference\"><a href=\"#cite_note-goodfellow:book16-12\"><span class=\"cite-bracket\">&#91;</span>12<span class=\"cite-bracket\">&#93;</span></a></sup> can achieve state-of-the-art results in many natural language tasks, e.g., in <a href=\"/wiki/Language_modeling\" class=\"mw-redirect\" title=\"Language modeling\">language modeling</a><sup id=\"cite_ref-jozefowicz:lm16_13-0\" class=\"reference\"><a href=\"#cite_note-jozefowicz:lm16-13\"><span class=\"cite-bracket\">&#91;</span>13<span class=\"cite-bracket\">&#93;</span></a></sup> and parsing.<sup id=\"cite_ref-choe:emnlp16_14-0\" class=\"reference\"><a href=\"#cite_note-choe:emnlp16-14\"><span class=\"cite-bracket\">&#91;</span>14<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-vinyals:nips15_15-0\" class=\"reference\"><a href=\"#cite_note-vinyals:nips15-15\"><span class=\"cite-bracket\">&#91;</span>15<span class=\"cite-bracket\">&#93;</span></a></sup> This is increasingly important <a href=\"/wiki/Artificial_intelligence_in_healthcare\" title=\"Artificial intelligence in healthcare\">in medicine and healthcare</a>, where NLP helps analyze notes and text in <a href=\"/wiki/Electronic_health_record\" title=\"Electronic health record\">electronic health records</a> that would otherwise be inaccessible for study when seeking to improve care<sup id=\"cite_ref-16\" class=\"reference\"><a href=\"#cite_note-16\"><span class=\"cite-bracket\">&#91;</span>16<span class=\"cite-bracket\">&#93;</span></a></sup> or protect patient privacy.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\"><span class=\"cite-bracket\">&#91;</span>17<span class=\"cite-bracket\">&#93;</span></a></sup>\n",
    "</p>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " class=\"mw-heading mw-heading3\"><h3 id=\"symbolic_nlp_(1950s_–_early_1990s)\"><span id=\"symbolic_nlp_.281950s_.e2.80.93_early_1990s.29\"></span>symbolic nlp (1950s – early 1990s)</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=natural_language_processing&amp;action=edit&amp;section=2\" title=\"edit section: symbolic nlp (1950s – early 1990s)\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
      "<p>the premise of symbolic nlp is well-summarized by <a href=\"/wiki/john_searle\" title=\"john searle\">john searle</a>'s <a href=\"/wiki/chinese_room\" title=\"chinese room\">chinese room</a> experiment: given a collection of rules (e.g., a chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other nlp tasks) by applying those rules to the data it confronts.\n",
      "</p>\n",
      "<ul><li><b>1950s</b>: the <a href=\"/wiki/georgetown-ibm_experiment\" class=\"mw-redirect\" title=\"georgetown-ibm experiment\">georgetown experiment</a> in 1954 involved fully <a href=\"/wiki/automatic_translation\" class=\"mw-redirect\" title=\"automatic translation\">automatic translation</a> of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\"><span class=\"cite-bracket\">&#91;</span>2<span class=\"cite-bracket\">&#93;</span></a></sup>  however, real progress was much slower, and after the <a href=\"/wiki/alpac\" title=\"alpac\">alpac report</a> in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america (though some research continued elsewhere, such as japan and europe<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\"><span class=\"cite-bracket\">&#91;</span>3<span class=\"cite-bracket\">&#93;</span></a></sup>) until the late 1980s when the first <a href=\"/wiki/statistical_machine_translation\" title=\"statistical machine translation\">statistical machine translation</a> systems were developed.</li>\n",
      "<li><b>1960s</b>: some notably successful natural language processing systems developed in the 1960s were <a href=\"/wiki/shrdlu\" title=\"shrdlu\">shrdlu</a>, a natural language system working in restricted \"<a href=\"/wiki/blocks_world\" title=\"blocks world\">blocks worlds</a>\" with restricted vocabularies, and <a href=\"/wiki/eliza\" title=\"eliza\">eliza</a>, a simulation of a <a href=\"/wiki/rogerian_psychotherapy\" class=\"mw-redirect\" title=\"rogerian psychotherapy\">rogerian psychotherapist</a>, written by <a href=\"/wiki/joseph_weizenbaum\" title=\"joseph weizenbaum\">joseph weizenbaum</a> between 1964 and 1966. using almost no information about human thought or emotion, eliza sometimes provided a startlingly human-like interaction. when the \"patient\" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \"my head hurts\" with \"why do you say your head hurts?\". <a href=\"/w/index.php?title=ross_quillian&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"ross quillian (page does not exist)\">ross quillian</a>'s successful work on natural language was demonstrated with a vocabulary of only <i>twenty</i> words, because that was all that would fit in a computer  memory at the time.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\"><span class=\"cite-bracket\">&#91;</span>4<span class=\"cite-bracket\">&#93;</span></a></sup></li></ul>\n",
      "<ul><li><b>1970s</b>: during the 1970s, many programmers began to write \"conceptual <a href=\"/wiki/ontology_(information_science)\" title=\"ontology (information science)\">ontologies</a>\", which structured real-world information into computer-understandable data.  examples are margie (schank, 1975), sam (cullingford, 1978), pam (wilensky, 1978), talespin (meehan, 1976), qualm (lehnert, 1977), politics (carbonell, 1979), and plot units (lehnert 1981).  during this time, the first <a href=\"/wiki/chatterbots\" class=\"mw-redirect\" title=\"chatterbots\">chatterbots</a> were written (e.g., <a href=\"/wiki/parry\" title=\"parry\">parry</a>).</li>\n",
      "<li><b>1980s</b>: the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule-based parsing (e.g., the development of <a href=\"/wiki/head-driven_phrase_structure_grammar\" title=\"head-driven phrase structure grammar\">hpsg</a> as a computational operationalization of <a href=\"/wiki/generative_grammar\" title=\"generative grammar\">generative grammar</a>), morphology (e.g., two-level morphology<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\"><span class=\"cite-bracket\">&#91;</span>5<span class=\"cite-bracket\">&#93;</span></a></sup>), semantics (e.g., <a href=\"/wiki/lesk_algorithm\" title=\"lesk algorithm\">lesk algorithm</a>), reference (e.g., within centering theory<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\"><span class=\"cite-bracket\">&#91;</span>6<span class=\"cite-bracket\">&#93;</span></a></sup>) and other areas of natural language understanding (e.g., in the <a href=\"/wiki/rhetorical_structure_theory\" title=\"rhetorical structure theory\">rhetorical structure theory</a>). other lines of research were continued, e.g., the development of chatterbots with <a href=\"/wiki/racter\" title=\"racter\">racter</a> and <a href=\"/wiki/jabberwacky\" title=\"jabberwacky\">jabberwacky</a>. an important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.<sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\"><span class=\"cite-bracket\">&#91;</span>7<span class=\"cite-bracket\">&#93;</span></a></sup></li></ul>\n",
      "<div class=\"mw-heading mw-heading3\"><h3 id=\"statistical_nlp_(1990s–2010s)\"><span id=\"statistical_nlp_.281990s.e2.80.932010s.29\"></span>statistical nlp (1990s–2010s)</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=natural_language_processing&amp;action=edit&amp;section=3\" title=\"edit section: statistical nlp (1990s–2010s)\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
      "<p>up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of <a href=\"/wiki/machine_learning\" title=\"machine learning\">machine learning</a> algorithms for language processing.  this was due to both the steady increase in computational power (see <a href=\"/wiki/moore%27s_law\" title=\"moore&#39;s law\">moore's law</a>) and the gradual lessening of the dominance of <a href=\"/wiki/noam_chomsky\" title=\"noam chomsky\">chomskyan</a> theories of linguistics (e.g. <a href=\"/wiki/transformational_grammar\" title=\"transformational grammar\">transformational grammar</a>), whose theoretical underpinnings discouraged the sort of <a href=\"/wiki/corpus_linguistics\" title=\"corpus linguistics\">corpus linguistics</a> that underlies the machine-learning approach to language processing.<sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\"><span class=\"cite-bracket\">&#91;</span>8<span class=\"cite-bracket\">&#93;</span></a></sup> \n",
      "</p>\n",
      "<ul><li><b>1990s</b>: many of the notable early successes in statistical methods in nlp occurred in the field of <a href=\"/wiki/machine_translation\" title=\"machine translation\">machine translation</a>, due especially to work at ibm research, such as <a href=\"/wiki/ibm_alignment_models\" title=\"ibm alignment models\">ibm alignment models</a>.  these systems were able to take advantage of existing multilingual <a href=\"/wiki/text_corpus\" title=\"text corpus\">textual corpora</a> that had been produced by the <a href=\"/wiki/parliament_of_canada\" title=\"parliament of canada\">parliament of canada</a> and the <a href=\"/wiki/european_union\" title=\"european union\">european union</a> as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  however, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. as a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.</li>\n",
      "<li><b>2000s</b>: with the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. research has thus increasingly focused on <a href=\"/wiki/unsupervised_learning\" title=\"unsupervised learning\">unsupervised</a> and <a href=\"/wiki/semi-supervised_learning\" class=\"mw-redirect\" title=\"semi-supervised learning\">semi-supervised learning</a> algorithms.  such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  generally, this task is much more difficult than <a href=\"/wiki/supervised_learning\" title=\"supervised learning\">supervised learning</a>, and typically produces less accurate results for a given amount of input data.  however, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the <a href=\"/wiki/world_wide_web\" title=\"world wide web\">world wide web</a>), which can often make up for the inferior results if the algorithm used has a low enough <a href=\"/wiki/time_complexity\" title=\"time complexity\">time complexity</a> to be practical.</li></ul>\n",
      "<div class=\"mw-heading mw-heading3\"><h3 id=\"neural_nlp_(present)\"><span id=\"neural_nlp_.28present.29\"></span>neural nlp (present)</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=natural_language_processing&amp;action=edit&amp;section=4\" title=\"edit section: neural nlp (present)\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\n",
      "<p>in 2003, <a href=\"/wiki/word_n-gram_language_model\" title=\"word n-gram language model\">word n-gram model</a>, at the time the best statistical algorithm, was outperformed by a <a href=\"/wiki/multi-layer_perceptron\" class=\"mw-redirect\" title=\"multi-layer perceptron\">multi-layer perceptron</a> (with a single hidden layer and context length of several words trained on up to 14 million of words with a cpu cluster in <a href=\"/wiki/language_model\" title=\"language model\">language modelling</a>) by <a href=\"/wiki/yoshua_bengio\" title=\"yoshua bengio\">yoshua bengio</a> with co-authors.<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\"><span class=\"cite-bracket\">&#91;</span>9<span class=\"cite-bracket\">&#93;</span></a></sup> \n",
      "</p><p>in 2010, <a href=\"/wiki/tom%c3%a1%c5%a1_mikolov\" title=\"tomáš mikolov\">tomáš mikolov</a> (then a phd student at <a href=\"/wiki/brno_university_of_technology\" title=\"brno university of technology\">brno university of technology</a>) with co-authors applied a simple <a href=\"/wiki/recurrent_neural_network\" title=\"recurrent neural network\">recurrent neural network</a> with a single hidden layer to language modelling,<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\"><span class=\"cite-bracket\">&#91;</span>10<span class=\"cite-bracket\">&#93;</span></a></sup> and in the following years he went on to develop <a href=\"/wiki/word2vec\" title=\"word2vec\">word2vec</a>. in the 2010s, <a href=\"/wiki/representation_learning\" class=\"mw-redirect\" title=\"representation learning\">representation learning</a> and <a href=\"/wiki/deep_learning\" title=\"deep learning\">deep neural network</a>-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. that popularity was due partly to a flurry of results showing that such techniques<sup id=\"cite_ref-goldberg:nnlp17_11-0\" class=\"reference\"><a href=\"#cite_note-goldberg:nnlp17-11\"><span class=\"cite-bracket\">&#91;</span>11<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-goodfellow:book16_12-0\" class=\"reference\"><a href=\"#cite_note-goodfellow:book16-12\"><span class=\"cite-bracket\">&#91;</span>12<span class=\"cite-bracket\">&#93;</span></a></sup> can achieve state-of-the-art results in many natural language tasks, e.g., in <a href=\"/wiki/language_modeling\" class=\"mw-redirect\" title=\"language modeling\">language modeling</a><sup id=\"cite_ref-jozefowicz:lm16_13-0\" class=\"reference\"><a href=\"#cite_note-jozefowicz:lm16-13\"><span class=\"cite-bracket\">&#91;</span>13<span class=\"cite-bracket\">&#93;</span></a></sup> and parsing.<sup id=\"cite_ref-choe:emnlp16_14-0\" class=\"reference\"><a href=\"#cite_note-choe:emnlp16-14\"><span class=\"cite-bracket\">&#91;</span>14<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-vinyals:nips15_15-0\" class=\"reference\"><a href=\"#cite_note-vinyals:nips15-15\"><span class=\"cite-bracket\">&#91;</span>15<span class=\"cite-bracket\">&#93;</span></a></sup> this is increasingly important <a href=\"/wiki/artificial_intelligence_in_healthcare\" title=\"artificial intelligence in healthcare\">in medicine and healthcare</a>, where nlp helps analyze notes and text in <a href=\"/wiki/electronic_health_record\" title=\"electronic health record\">electronic health records</a> that would otherwise be inaccessible for study when seeking to improve care<sup id=\"cite_ref-16\" class=\"reference\"><a href=\"#cite_note-16\"><span class=\"cite-bracket\">&#91;</span>16<span class=\"cite-bracket\">&#93;</span></a></sup> or protect patient privacy.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\"><span class=\"cite-bracket\">&#91;</span>17<span class=\"cite-bracket\">&#93;</span></a></sup>\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "df=df.lower()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " class=\"mw-heading mw-heading3\">symbolic nlp (1950s – early 1990s)[edit]\n",
      "the premise of symbolic nlp is well-summarized by john searle's chinese room experiment: given a collection of rules (e.g., a chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other nlp tasks) by applying those rules to the data it confronts.\n",
      "\n",
      "1950s: the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem.&#91;2&#93;  however, real progress was much slower, and after the alpac report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america (though some research continued elsewhere, such as japan and europe&#91;3&#93;) until the late 1980s when the first statistical machine translation systems were developed.\n",
      "1960s: some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapist, written by joseph weizenbaum between 1964 and 1966. using almost no information about human thought or emotion, eliza sometimes provided a startlingly human-like interaction. when the \"patient\" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \"my head hurts\" with \"why do you say your head hurts?\". ross quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer  memory at the time.&#91;4&#93;\n",
      "1970s: during the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.  examples are margie (schank, 1975), sam (cullingford, 1978), pam (wilensky, 1978), talespin (meehan, 1976), qualm (lehnert, 1977), politics (carbonell, 1979), and plot units (lehnert 1981).  during this time, the first chatterbots were written (e.g., parry).\n",
      "1980s: the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule-based parsing (e.g., the development of hpsg as a computational operationalization of generative grammar), morphology (e.g., two-level morphology&#91;5&#93;), semantics (e.g., lesk algorithm), reference (e.g., within centering theory&#91;6&#93;) and other areas of natural language understanding (e.g., in the rhetorical structure theory). other lines of research were continued, e.g., the development of chatterbots with racter and jabberwacky. an important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.&#91;7&#93;\n",
      "statistical nlp (1990s–2010s)[edit]\n",
      "up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  this was due to both the steady increase in computational power (see moore's law) and the gradual lessening of the dominance of chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.&#91;8&#93; \n",
      "\n",
      "1990s: many of the notable early successes in statistical methods in nlp occurred in the field of machine translation, due especially to work at ibm research, such as ibm alignment models.  these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  however, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. as a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n",
      "2000s: with the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. research has thus increasingly focused on unsupervised and semi-supervised learning algorithms.  such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.  however, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the world wide web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.\n",
      "neural nlp (present)[edit]\n",
      "in 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a cpu cluster in language modelling) by yoshua bengio with co-authors.&#91;9&#93; \n",
      "in 2010, tomáš mikolov (then a phd student at brno university of technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,&#91;10&#93; and in the following years he went on to develop word2vec. in the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. that popularity was due partly to a flurry of results showing that such techniques&#91;11&#93;&#91;12&#93; can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling&#91;13&#93; and parsing.&#91;14&#93;&#91;15&#93; this is increasingly important in medicine and healthcare, where nlp helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care&#91;16&#93; or protect patient privacy.&#91;17&#93;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "df=remove_html_tags(df)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " class=\"mw-heading mw-heading3\">symbolic nlp (1950s – early 1990s)[edit]\n",
      "the premise of symbolic nlp is well-summarized by john searle's chinese room experiment: given a collection of rules (e.g., a chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other nlp tasks) by applying those rules to the data it confronts.\n",
      "\n",
      "1950s: the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem.&#91;2&#93;  however, real progress was much slower, and after the alpac report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america (though some research continued elsewhere, such as japan and europe&#91;3&#93;) until the late 1980s when the first statistical machine translation systems were developed.\n",
      "1960s: some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapist, written by joseph weizenbaum between 1964 and 1966. using almost no information about human thought or emotion, eliza sometimes provided a startlingly human-like interaction. when the \"patient\" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \"my head hurts\" with \"why do you say your head hurts?\". ross quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer  memory at the time.&#91;4&#93;\n",
      "1970s: during the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.  examples are margie (schank, 1975), sam (cullingford, 1978), pam (wilensky, 1978), talespin (meehan, 1976), qualm (lehnert, 1977), politics (carbonell, 1979), and plot units (lehnert 1981).  during this time, the first chatterbots were written (e.g., parry).\n",
      "1980s: the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule-based parsing (e.g., the development of hpsg as a computational operationalization of generative grammar), morphology (e.g., two-level morphology&#91;5&#93;), semantics (e.g., lesk algorithm), reference (e.g., within centering theory&#91;6&#93;) and other areas of natural language understanding (e.g., in the rhetorical structure theory). other lines of research were continued, e.g., the development of chatterbots with racter and jabberwacky. an important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.&#91;7&#93;\n",
      "statistical nlp (1990s–2010s)[edit]\n",
      "up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  this was due to both the steady increase in computational power (see moore's law) and the gradual lessening of the dominance of chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.&#91;8&#93; \n",
      "\n",
      "1990s: many of the notable early successes in statistical methods in nlp occurred in the field of machine translation, due especially to work at ibm research, such as ibm alignment models.  these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  however, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. as a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n",
      "2000s: with the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. research has thus increasingly focused on unsupervised and semi-supervised learning algorithms.  such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.  however, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the world wide web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.\n",
      "neural nlp (present)[edit]\n",
      "in 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a cpu cluster in language modelling) by yoshua bengio with co-authors.&#91;9&#93; \n",
      "in 2010, tomáš mikolov (then a phd student at brno university of technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,&#91;10&#93; and in the following years he went on to develop word2vec. in the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. that popularity was due partly to a flurry of results showing that such techniques&#91;11&#93;&#91;12&#93; can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling&#91;13&#93; and parsing.&#91;14&#93;&#91;15&#93; this is increasingly important in medicine and healthcare, where nlp helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care&#91;16&#93; or protect patient privacy.&#91;17&#93;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "df= remove_url(df)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " classmwheading mwheading3symbolic nlp 1950s – early 1990sedit\n",
      "the premise of symbolic nlp is wellsummarized by john searles chinese room experiment given a collection of rules eg a chinese phrasebook with questions and matching answers the computer emulates natural language understanding or other nlp tasks by applying those rules to the data it confronts\n",
      "\n",
      "1950s the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english the authors claimed that within three or five years machine translation would be a solved problem91293  however real progress was much slower and after the alpac report in 1966 which found that ten years of research had failed to fulfill the expectations funding for machine translation was dramatically reduced little further research in machine translation was conducted in america though some research continued elsewhere such as japan and europe91393 until the late 1980s when the first statistical machine translation systems were developed\n",
      "1960s some notably successful natural language processing systems developed in the 1960s were shrdlu a natural language system working in restricted blocks worlds with restricted vocabularies and eliza a simulation of a rogerian psychotherapist written by joseph weizenbaum between 1964 and 1966 using almost no information about human thought or emotion eliza sometimes provided a startlingly humanlike interaction when the patient exceeded the very small knowledge base eliza might provide a generic response for example responding to my head hurts with why do you say your head hurts ross quillians successful work on natural language was demonstrated with a vocabulary of only twenty words because that was all that would fit in a computer  memory at the time91493\n",
      "1970s during the 1970s many programmers began to write conceptual ontologies which structured realworld information into computerunderstandable data  examples are margie schank 1975 sam cullingford 1978 pam wilensky 1978 talespin meehan 1976 qualm lehnert 1977 politics carbonell 1979 and plot units lehnert 1981  during this time the first chatterbots were written eg parry\n",
      "1980s the 1980s and early 1990s mark the heyday of symbolic methods in nlp focus areas of the time included research on rulebased parsing eg the development of hpsg as a computational operationalization of generative grammar morphology eg twolevel morphology91593 semantics eg lesk algorithm reference eg within centering theory91693 and other areas of natural language understanding eg in the rhetorical structure theory other lines of research were continued eg the development of chatterbots with racter and jabberwacky an important development that eventually led to the statistical turn in the 1990s was the rising importance of quantitative evaluation in this period91793\n",
      "statistical nlp 1990s–2010sedit\n",
      "up until the 1980s most natural language processing systems were based on complex sets of handwritten rules  starting in the late 1980s however there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing  this was due to both the steady increase in computational power see moores law and the gradual lessening of the dominance of chomskyan theories of linguistics eg transformational grammar whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machinelearning approach to language processing91893 \n",
      "\n",
      "1990s many of the notable early successes in statistical methods in nlp occurred in the field of machine translation due especially to work at ibm research such as ibm alignment models  these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government  however most other systems depended on corpora specifically developed for the tasks implemented by these systems which was and often continues to be a major limitation in the success of these systems as a result a great deal of research has gone into methods of more effectively learning from limited amounts of data\n",
      "2000s with the growth of the web increasing amounts of raw unannotated language data have become available since the mid1990s research has thus increasingly focused on unsupervised and semisupervised learning algorithms  such algorithms can learn from data that has not been handannotated with the desired answers or using a combination of annotated and nonannotated data  generally this task is much more difficult than supervised learning and typically produces less accurate results for a given amount of input data  however there is an enormous amount of nonannotated data available including among other things the entire content of the world wide web which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical\n",
      "neural nlp presentedit\n",
      "in 2003 word ngram model at the time the best statistical algorithm was outperformed by a multilayer perceptron with a single hidden layer and context length of several words trained on up to 14 million of words with a cpu cluster in language modelling by yoshua bengio with coauthors91993 \n",
      "in 2010 tomáš mikolov then a phd student at brno university of technology with coauthors applied a simple recurrent neural network with a single hidden layer to language modelling911093 and in the following years he went on to develop word2vec in the 2010s representation learning and deep neural networkstyle featuring many hidden layers machine learning methods became widespread in natural language processing that popularity was due partly to a flurry of results showing that such techniques911193911293 can achieve stateoftheart results in many natural language tasks eg in language modeling911393 and parsing911493911593 this is increasingly important in medicine and healthcare where nlp helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care911693 or protect patient privacy911793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation\n",
    "\n",
    "punct=string.punctuation\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('', '', punct))\n",
    "\n",
    "df=remove_punc1(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classmwheading mwheading3symbolic nlp 1950s – early 1990sedit premise symbolic nlp wellsummarized john searles chinese room experiment given collection rules eg chinese phrasebook questions matching answers computer emulates natural language understanding nlp tasks applying rules data confronts 1950s georgetown experiment 1954 involved fully automatic translation sixty russian sentences english authors claimed within three five years machine translation would solved problem91293 however real progress much slower alpac report 1966 found ten years research failed fulfill expectations funding machine translation dramatically reduced little research machine translation conducted america though research continued elsewhere japan europe91393 late 1980s first statistical machine translation systems developed 1960s notably successful natural language processing systems developed 1960s shrdlu natural language system working restricted blocks worlds restricted vocabularies eliza simulation rogerian psychotherapist written joseph weizenbaum 1964 1966 using almost information human thought emotion eliza sometimes provided startlingly humanlike interaction patient exceeded small knowledge base eliza might provide generic response example responding head hurts say head hurts ross quillians successful work natural language demonstrated vocabulary twenty words would fit computer memory time91493 1970s 1970s many programmers began write conceptual ontologies structured realworld information computerunderstandable data examples margie schank 1975 sam cullingford 1978 pam wilensky 1978 talespin meehan 1976 qualm lehnert 1977 politics carbonell 1979 plot units lehnert 1981 time first chatterbots written eg parry 1980s 1980s early 1990s mark heyday symbolic methods nlp focus areas time included research rulebased parsing eg development hpsg computational operationalization generative grammar morphology eg twolevel morphology91593 semantics eg lesk algorithm reference eg within centering theory91693 areas natural language understanding eg rhetorical structure theory lines research continued eg development chatterbots racter jabberwacky important development eventually led statistical turn 1990s rising importance quantitative evaluation period91793 statistical nlp 1990s–2010sedit 1980s natural language processing systems based complex sets handwritten rules starting late 1980s however revolution natural language processing introduction machine learning algorithms language processing due steady increase computational power see moores law gradual lessening dominance chomskyan theories linguistics eg transformational grammar whose theoretical underpinnings discouraged sort corpus linguistics underlies machinelearning approach language processing91893 1990s many notable early successes statistical methods nlp occurred field machine translation due especially work ibm research ibm alignment models systems able take advantage existing multilingual textual corpora produced parliament canada european union result laws calling translation governmental proceedings official languages corresponding systems government however systems depended corpora specifically developed tasks implemented systems often continues major limitation success systems result great deal research gone methods effectively learning limited amounts data 2000s growth web increasing amounts raw unannotated language data become available since mid1990s research thus increasingly focused unsupervised semisupervised learning algorithms algorithms learn data handannotated desired answers using combination annotated nonannotated data generally task much difficult supervised learning typically produces less accurate results given amount input data however enormous amount nonannotated data available including among things entire content world wide web often make inferior results algorithm used low enough time complexity practical neural nlp presentedit 2003 word ngram model time best statistical algorithm outperformed multilayer perceptron single hidden layer context length several words trained 14 million words cpu cluster language modelling yoshua bengio coauthors91993 2010 tomáš mikolov phd student brno university technology coauthors applied simple recurrent neural network single hidden layer language modelling911093 following years went develop word2vec 2010s representation learning deep neural networkstyle featuring many hidden layers machine learning methods became widespread natural language processing popularity due partly flurry results showing techniques911193911293 achieve stateoftheart results many natural language tasks eg language modeling911393 parsing911493911593 increasingly important medicine healthcare nlp helps analyze notes text electronic health records would otherwise inaccessible study seeking improve care911693 protect patient privacy911793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hrish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df = ' '.join([word for word in df.split() if word not in (stop)])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classmwhead mwheading3symbol nlp 1950 – earli 1990sedit premis symbol nlp wellsummar john searl chines room experi given collect rule eg chines phrasebook question match answer comput emul natur languag understand nlp task appli rule data confront 1950 georgetown experi 1954 involv fulli automat translat sixti russian sentenc english author claim within three five year machin translat would solv problem91293 howev real progress much slower alpac report 1966 found ten year research fail fulfil expect fund machin translat dramat reduc littl research machin translat conduct america though research continu elsewher japan europe91393 late 1980 first statist machin translat system develop 1960 notabl success natur languag process system develop 1960 shrdlu natur languag system work restrict block world restrict vocabulari eliza simul rogerian psychotherapist written joseph weizenbaum 1964 1966 use almost inform human thought emot eliza sometim provid startlingli humanlik interact patient exceed small knowledg base eliza might provid gener respons exampl respond head hurt say head hurt ross quillian success work natur languag demonstr vocabulari twenti word would fit comput memori time91493 1970 1970 mani programm began write conceptu ontolog structur realworld inform computerunderstand data exampl margi schank 1975 sam cullingford 1978 pam wilenski 1978 talespin meehan 1976 qualm lehnert 1977 polit carbonel 1979 plot unit lehnert 1981 time first chatterbot written eg parri 1980 1980 earli 1990 mark heyday symbol method nlp focu area time includ research rulebas pars eg develop hpsg comput operation gener grammar morpholog eg twolevel morphology91593 semant eg lesk algorithm refer eg within center theory91693 area natur languag understand eg rhetor structur theori line research continu eg develop chatterbot racter jabberwacki import develop eventu led statist turn 1990 rise import quantit evalu period91793 statist nlp 1990s–2010sedit 1980 natur languag process system base complex set handwritten rule start late 1980 howev revolut natur languag process introduct machin learn algorithm languag process due steadi increas comput power see moor law gradual lessen domin chomskyan theori linguist eg transform grammar whose theoret underpin discourag sort corpu linguist underli machinelearn approach languag processing91893 1990 mani notabl earli success statist method nlp occur field machin translat due especi work ibm research ibm align model system abl take advantag exist multilingu textual corpora produc parliament canada european union result law call translat government proceed offici languag correspond system govern howev system depend corpora specif develop task implement system often continu major limit success system result great deal research gone method effect learn limit amount data 2000 growth web increas amount raw unannot languag data becom avail sinc mid1990 research thu increasingli focus unsupervis semisupervis learn algorithm algorithm learn data handannot desir answer use combin annot nonannot data gener task much difficult supervis learn typic produc less accur result given amount input data howev enorm amount nonannot data avail includ among thing entir content world wide web often make inferior result algorithm use low enough time complex practic neural nlp presentedit 2003 word ngram model time best statist algorithm outperform multilay perceptron singl hidden layer context length sever word train 14 million word cpu cluster languag model yoshua bengio coauthors91993 2010 tomáš mikolov phd student brno univers technolog coauthor appli simpl recurr neural network singl hidden layer languag modelling911093 follow year went develop word2vec 2010 represent learn deep neural networkstyl featur mani hidden layer machin learn method becam widespread natur languag process popular due partli flurri result show techniques911193911293 achiev stateoftheart result mani natur languag task eg languag modeling911393 parsing911493911593 increasingli import medicin healthcar nlp help analyz note text electron health record would otherwis inaccess studi seek improv care911693 protect patient privacy911793'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "stemwords=stem_words(df)\n",
    "stemwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classmwheading',\n",
       " 'mwheading3symbolic',\n",
       " 'nlp',\n",
       " '1950s',\n",
       " '–',\n",
       " 'early',\n",
       " '1990sedit',\n",
       " 'premise',\n",
       " 'symbolic',\n",
       " 'nlp',\n",
       " 'wellsummarized',\n",
       " 'john',\n",
       " 'searles',\n",
       " 'chinese',\n",
       " 'room',\n",
       " 'experiment',\n",
       " 'given',\n",
       " 'collection',\n",
       " 'rules',\n",
       " 'eg',\n",
       " 'chinese',\n",
       " 'phrasebook',\n",
       " 'questions',\n",
       " 'matching',\n",
       " 'answers',\n",
       " 'computer',\n",
       " 'emulates',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'nlp',\n",
       " 'tasks',\n",
       " 'applying',\n",
       " 'rules',\n",
       " 'data',\n",
       " 'confronts',\n",
       " '1950s',\n",
       " 'georgetown',\n",
       " 'experiment',\n",
       " '1954',\n",
       " 'involved',\n",
       " 'fully',\n",
       " 'automatic',\n",
       " 'translation',\n",
       " 'sixty',\n",
       " 'russian',\n",
       " 'sentences',\n",
       " 'english',\n",
       " 'authors',\n",
       " 'claimed',\n",
       " 'within',\n",
       " 'three',\n",
       " 'five',\n",
       " 'years',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'would',\n",
       " 'solved',\n",
       " 'problem91293',\n",
       " 'however',\n",
       " 'real',\n",
       " 'progress',\n",
       " 'much',\n",
       " 'slower',\n",
       " 'alpac',\n",
       " 'report',\n",
       " '1966',\n",
       " 'found',\n",
       " 'ten',\n",
       " 'years',\n",
       " 'research',\n",
       " 'failed',\n",
       " 'fulfill',\n",
       " 'expectations',\n",
       " 'funding',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'dramatically',\n",
       " 'reduced',\n",
       " 'little',\n",
       " 'research',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'conducted',\n",
       " 'america',\n",
       " 'though',\n",
       " 'research',\n",
       " 'continued',\n",
       " 'elsewhere',\n",
       " 'japan',\n",
       " 'europe91393',\n",
       " 'late',\n",
       " '1980s',\n",
       " 'first',\n",
       " 'statistical',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'systems',\n",
       " 'developed',\n",
       " '1960s',\n",
       " 'notably',\n",
       " 'successful',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'developed',\n",
       " '1960s',\n",
       " 'shrdlu',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'system',\n",
       " 'working',\n",
       " 'restricted',\n",
       " 'blocks',\n",
       " 'worlds',\n",
       " 'restricted',\n",
       " 'vocabularies',\n",
       " 'eliza',\n",
       " 'simulation',\n",
       " 'rogerian',\n",
       " 'psychotherapist',\n",
       " 'written',\n",
       " 'joseph',\n",
       " 'weizenbaum',\n",
       " '1964',\n",
       " '1966',\n",
       " 'using',\n",
       " 'almost',\n",
       " 'information',\n",
       " 'human',\n",
       " 'thought',\n",
       " 'emotion',\n",
       " 'eliza',\n",
       " 'sometimes',\n",
       " 'provided',\n",
       " 'startlingly',\n",
       " 'humanlike',\n",
       " 'interaction',\n",
       " 'patient',\n",
       " 'exceeded',\n",
       " 'small',\n",
       " 'knowledge',\n",
       " 'base',\n",
       " 'eliza',\n",
       " 'might',\n",
       " 'provide',\n",
       " 'generic',\n",
       " 'response',\n",
       " 'example',\n",
       " 'responding',\n",
       " 'head',\n",
       " 'hurts',\n",
       " 'say',\n",
       " 'head',\n",
       " 'hurts',\n",
       " 'ross',\n",
       " 'quillians',\n",
       " 'successful',\n",
       " 'work',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'demonstrated',\n",
       " 'vocabulary',\n",
       " 'twenty',\n",
       " 'words',\n",
       " 'would',\n",
       " 'fit',\n",
       " 'computer',\n",
       " 'memory',\n",
       " 'time91493',\n",
       " '1970s',\n",
       " '1970s',\n",
       " 'many',\n",
       " 'programmers',\n",
       " 'began',\n",
       " 'write',\n",
       " 'conceptual',\n",
       " 'ontologies',\n",
       " 'structured',\n",
       " 'realworld',\n",
       " 'information',\n",
       " 'computerunderstandable',\n",
       " 'data',\n",
       " 'examples',\n",
       " 'margie',\n",
       " 'schank',\n",
       " '1975',\n",
       " 'sam',\n",
       " 'cullingford',\n",
       " '1978',\n",
       " 'pam',\n",
       " 'wilensky',\n",
       " '1978',\n",
       " 'talespin',\n",
       " 'meehan',\n",
       " '1976',\n",
       " 'qualm',\n",
       " 'lehnert',\n",
       " '1977',\n",
       " 'politics',\n",
       " 'carbonell',\n",
       " '1979',\n",
       " 'plot',\n",
       " 'units',\n",
       " 'lehnert',\n",
       " '1981',\n",
       " 'time',\n",
       " 'first',\n",
       " 'chatterbots',\n",
       " 'written',\n",
       " 'eg',\n",
       " 'parry',\n",
       " '1980s',\n",
       " '1980s',\n",
       " 'early',\n",
       " '1990s',\n",
       " 'mark',\n",
       " 'heyday',\n",
       " 'symbolic',\n",
       " 'methods',\n",
       " 'nlp',\n",
       " 'focus',\n",
       " 'areas',\n",
       " 'time',\n",
       " 'included',\n",
       " 'research',\n",
       " 'rulebased',\n",
       " 'parsing',\n",
       " 'eg',\n",
       " 'development',\n",
       " 'hpsg',\n",
       " 'computational',\n",
       " 'operationalization',\n",
       " 'generative',\n",
       " 'grammar',\n",
       " 'morphology',\n",
       " 'eg',\n",
       " 'twolevel',\n",
       " 'morphology91593',\n",
       " 'semantics',\n",
       " 'eg',\n",
       " 'lesk',\n",
       " 'algorithm',\n",
       " 'reference',\n",
       " 'eg',\n",
       " 'within',\n",
       " 'centering',\n",
       " 'theory91693',\n",
       " 'areas',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'eg',\n",
       " 'rhetorical',\n",
       " 'structure',\n",
       " 'theory',\n",
       " 'lines',\n",
       " 'research',\n",
       " 'continued',\n",
       " 'eg',\n",
       " 'development',\n",
       " 'chatterbots',\n",
       " 'racter',\n",
       " 'jabberwacky',\n",
       " 'important',\n",
       " 'development',\n",
       " 'eventually',\n",
       " 'led',\n",
       " 'statistical',\n",
       " 'turn',\n",
       " '1990s',\n",
       " 'rising',\n",
       " 'importance',\n",
       " 'quantitative',\n",
       " 'evaluation',\n",
       " 'period91793',\n",
       " 'statistical',\n",
       " 'nlp',\n",
       " '1990s–2010sedit',\n",
       " '1980s',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'based',\n",
       " 'complex',\n",
       " 'sets',\n",
       " 'handwritten',\n",
       " 'rules',\n",
       " 'starting',\n",
       " 'late',\n",
       " '1980s',\n",
       " 'however',\n",
       " 'revolution',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'introduction',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'due',\n",
       " 'steady',\n",
       " 'increase',\n",
       " 'computational',\n",
       " 'power',\n",
       " 'see',\n",
       " 'moores',\n",
       " 'law',\n",
       " 'gradual',\n",
       " 'lessening',\n",
       " 'dominance',\n",
       " 'chomskyan',\n",
       " 'theories',\n",
       " 'linguistics',\n",
       " 'eg',\n",
       " 'transformational',\n",
       " 'grammar',\n",
       " 'whose',\n",
       " 'theoretical',\n",
       " 'underpinnings',\n",
       " 'discouraged',\n",
       " 'sort',\n",
       " 'corpus',\n",
       " 'linguistics',\n",
       " 'underlies',\n",
       " 'machinelearning',\n",
       " 'approach',\n",
       " 'language',\n",
       " 'processing91893',\n",
       " '1990s',\n",
       " 'many',\n",
       " 'notable',\n",
       " 'early',\n",
       " 'successes',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'nlp',\n",
       " 'occurred',\n",
       " 'field',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'due',\n",
       " 'especially',\n",
       " 'work',\n",
       " 'ibm',\n",
       " 'research',\n",
       " 'ibm',\n",
       " 'alignment',\n",
       " 'models',\n",
       " 'systems',\n",
       " 'able',\n",
       " 'take',\n",
       " 'advantage',\n",
       " 'existing',\n",
       " 'multilingual',\n",
       " 'textual',\n",
       " 'corpora',\n",
       " 'produced',\n",
       " 'parliament',\n",
       " 'canada',\n",
       " 'european',\n",
       " 'union',\n",
       " 'result',\n",
       " 'laws',\n",
       " 'calling',\n",
       " 'translation',\n",
       " 'governmental',\n",
       " 'proceedings',\n",
       " 'official',\n",
       " 'languages',\n",
       " 'corresponding',\n",
       " 'systems',\n",
       " 'government',\n",
       " 'however',\n",
       " 'systems',\n",
       " 'depended',\n",
       " 'corpora',\n",
       " 'specifically',\n",
       " 'developed',\n",
       " 'tasks',\n",
       " 'implemented',\n",
       " 'systems',\n",
       " 'often',\n",
       " 'continues',\n",
       " 'major',\n",
       " 'limitation',\n",
       " 'success',\n",
       " 'systems',\n",
       " 'result',\n",
       " 'great',\n",
       " 'deal',\n",
       " 'research',\n",
       " 'gone',\n",
       " 'methods',\n",
       " 'effectively',\n",
       " 'learning',\n",
       " 'limited',\n",
       " 'amounts',\n",
       " 'data',\n",
       " '2000s',\n",
       " 'growth',\n",
       " 'web',\n",
       " 'increasing',\n",
       " 'amounts',\n",
       " 'raw',\n",
       " 'unannotated',\n",
       " 'language',\n",
       " 'data',\n",
       " 'become',\n",
       " 'available',\n",
       " 'since',\n",
       " 'mid1990s',\n",
       " 'research',\n",
       " 'thus',\n",
       " 'increasingly',\n",
       " 'focused',\n",
       " 'unsupervised',\n",
       " 'semisupervised',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'algorithms',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'handannotated',\n",
       " 'desired',\n",
       " 'answers',\n",
       " 'using',\n",
       " 'combination',\n",
       " 'annotated',\n",
       " 'nonannotated',\n",
       " 'data',\n",
       " 'generally',\n",
       " 'task',\n",
       " 'much',\n",
       " 'difficult',\n",
       " 'supervised',\n",
       " 'learning',\n",
       " 'typically',\n",
       " 'produces',\n",
       " 'less',\n",
       " 'accurate',\n",
       " 'results',\n",
       " 'given',\n",
       " 'amount',\n",
       " 'input',\n",
       " 'data',\n",
       " 'however',\n",
       " 'enormous',\n",
       " 'amount',\n",
       " 'nonannotated',\n",
       " 'data',\n",
       " 'available',\n",
       " 'including',\n",
       " 'among',\n",
       " 'things',\n",
       " 'entire',\n",
       " 'content',\n",
       " 'world',\n",
       " 'wide',\n",
       " 'web',\n",
       " 'often',\n",
       " 'make',\n",
       " 'inferior',\n",
       " 'results',\n",
       " 'algorithm',\n",
       " 'used',\n",
       " 'low',\n",
       " 'enough',\n",
       " 'time',\n",
       " 'complexity',\n",
       " 'practical',\n",
       " 'neural',\n",
       " 'nlp',\n",
       " 'presentedit',\n",
       " '2003',\n",
       " 'word',\n",
       " 'ngram',\n",
       " 'model',\n",
       " 'time',\n",
       " 'best',\n",
       " 'statistical',\n",
       " 'algorithm',\n",
       " 'outperformed',\n",
       " 'multilayer',\n",
       " 'perceptron',\n",
       " 'single',\n",
       " 'hidden',\n",
       " 'layer',\n",
       " 'context',\n",
       " 'length',\n",
       " 'several',\n",
       " 'words',\n",
       " 'trained',\n",
       " '14',\n",
       " 'million',\n",
       " 'words',\n",
       " 'cpu',\n",
       " 'cluster',\n",
       " 'language',\n",
       " 'modelling',\n",
       " 'yoshua',\n",
       " 'bengio',\n",
       " 'coauthors91993',\n",
       " '2010',\n",
       " 'tomáš',\n",
       " 'mikolov',\n",
       " 'phd',\n",
       " 'student',\n",
       " 'brno',\n",
       " 'university',\n",
       " 'technology',\n",
       " 'coauthors',\n",
       " 'applied',\n",
       " 'simple',\n",
       " 'recurrent',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'single',\n",
       " 'hidden',\n",
       " 'layer',\n",
       " 'language',\n",
       " 'modelling911093',\n",
       " 'following',\n",
       " 'years',\n",
       " 'went',\n",
       " 'develop',\n",
       " 'word2vec',\n",
       " '2010s',\n",
       " 'representation',\n",
       " 'learning',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'networkstyle',\n",
       " 'featuring',\n",
       " 'many',\n",
       " 'hidden',\n",
       " 'layers',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'methods',\n",
       " 'became',\n",
       " 'widespread',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'popularity',\n",
       " 'due',\n",
       " 'partly',\n",
       " 'flurry',\n",
       " 'results',\n",
       " 'showing',\n",
       " 'techniques911193911293',\n",
       " 'achieve',\n",
       " 'stateoftheart',\n",
       " 'results',\n",
       " 'many',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'tasks',\n",
       " 'eg',\n",
       " 'language',\n",
       " 'modeling911393',\n",
       " 'parsing911493911593',\n",
       " 'increasingly',\n",
       " 'important',\n",
       " 'medicine',\n",
       " 'healthcare',\n",
       " 'nlp',\n",
       " 'helps',\n",
       " 'analyze',\n",
       " 'notes',\n",
       " 'text',\n",
       " 'electronic',\n",
       " 'health',\n",
       " 'records',\n",
       " 'would',\n",
       " 'otherwise',\n",
       " 'inaccessible',\n",
       " 'study',\n",
       " 'seeking',\n",
       " 'improve',\n",
       " 'care911693',\n",
       " 'protect',\n",
       " 'patient',\n",
       " 'privacy911793']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = df\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classmwheading      classmwheading      \n",
      "mwheading3symbolic  mwheading3symbolic  \n",
      "nlp                 nlp                 \n",
      "1950s               1950s               \n",
      "–                   –                   \n",
      "early               early               \n",
      "1990sedit           1990sedit           \n",
      "premise             premise             \n",
      "symbolic            symbolic            \n",
      "nlp                 nlp                 \n",
      "wellsummarized      wellsummarized      \n",
      "john                john                \n",
      "searles             searles             \n",
      "chinese             chinese             \n",
      "room                room                \n",
      "experiment          experiment          \n",
      "given               give                \n",
      "collection          collection          \n",
      "rules               rule                \n",
      "eg                  eg                  \n",
      "chinese             chinese             \n",
      "phrasebook          phrasebook          \n",
      "questions           question            \n",
      "matching            match               \n",
      "answers             answer              \n",
      "computer            computer            \n",
      "emulates            emulate             \n",
      "natural             natural             \n",
      "language            language            \n",
      "understanding       understand          \n",
      "nlp                 nlp                 \n",
      "tasks               task                \n",
      "applying            apply               \n",
      "rules               rule                \n",
      "data                data                \n",
      "confronts           confront            \n",
      "1950s               1950s               \n",
      "georgetown          georgetown          \n",
      "experiment          experiment          \n",
      "1954                1954                \n",
      "involved            involve             \n",
      "fully               fully               \n",
      "automatic           automatic           \n",
      "translation         translation         \n",
      "sixty               sixty               \n",
      "russian             russian             \n",
      "sentences           sentence            \n",
      "english             english             \n",
      "authors             author              \n",
      "claimed             claim               \n",
      "within              within              \n",
      "three               three               \n",
      "five                five                \n",
      "years               years               \n",
      "machine             machine             \n",
      "translation         translation         \n",
      "would               would               \n",
      "solved              solve               \n",
      "problem91293        problem91293        \n",
      "however             however             \n",
      "real                real                \n",
      "progress            progress            \n",
      "much                much                \n",
      "slower              slower              \n",
      "alpac               alpac               \n",
      "report              report              \n",
      "1966                1966                \n",
      "found               find                \n",
      "ten                 ten                 \n",
      "years               years               \n",
      "research            research            \n",
      "failed              fail                \n",
      "fulfill             fulfill             \n",
      "expectations        expectations        \n",
      "funding             fund                \n",
      "machine             machine             \n",
      "translation         translation         \n",
      "dramatically        dramatically        \n",
      "reduced             reduce              \n",
      "little              little              \n",
      "research            research            \n",
      "machine             machine             \n",
      "translation         translation         \n",
      "conducted           conduct             \n",
      "america             america             \n",
      "though              though              \n",
      "research            research            \n",
      "continued           continue            \n",
      "elsewhere           elsewhere           \n",
      "japan               japan               \n",
      "europe91393         europe91393         \n",
      "late                late                \n",
      "1980s               1980s               \n",
      "first               first               \n",
      "statistical         statistical         \n",
      "machine             machine             \n",
      "translation         translation         \n",
      "systems             systems             \n",
      "developed           develop             \n",
      "1960s               1960s               \n",
      "notably             notably             \n",
      "successful          successful          \n",
      "natural             natural             \n",
      "language            language            \n",
      "processing          process             \n",
      "systems             systems             \n",
      "developed           develop             \n",
      "1960s               1960s               \n",
      "shrdlu              shrdlu              \n",
      "natural             natural             \n",
      "language            language            \n",
      "system              system              \n",
      "working             work                \n",
      "restricted          restrict            \n",
      "blocks              block               \n",
      "worlds              worlds              \n",
      "restricted          restrict            \n",
      "vocabularies        vocabularies        \n",
      "eliza               eliza               \n",
      "simulation          simulation          \n",
      "rogerian            rogerian            \n",
      "psychotherapist     psychotherapist     \n",
      "written             write               \n",
      "joseph              joseph              \n",
      "weizenbaum          weizenbaum          \n",
      "1964                1964                \n",
      "1966                1966                \n",
      "using               use                 \n",
      "almost              almost              \n",
      "information         information         \n",
      "human               human               \n",
      "thought             think               \n",
      "emotion             emotion             \n",
      "eliza               eliza               \n",
      "sometimes           sometimes           \n",
      "provided            provide             \n",
      "startlingly         startlingly         \n",
      "humanlike           humanlike           \n",
      "interaction         interaction         \n",
      "patient             patient             \n",
      "exceeded            exceed              \n",
      "small               small               \n",
      "knowledge           knowledge           \n",
      "base                base                \n",
      "eliza               eliza               \n",
      "might               might               \n",
      "provide             provide             \n",
      "generic             generic             \n",
      "response            response            \n",
      "example             example             \n",
      "responding          respond             \n",
      "head                head                \n",
      "hurts               hurt                \n",
      "say                 say                 \n",
      "head                head                \n",
      "hurts               hurt                \n",
      "ross                ross                \n",
      "quillians           quillians           \n",
      "successful          successful          \n",
      "work                work                \n",
      "natural             natural             \n",
      "language            language            \n",
      "demonstrated        demonstrate         \n",
      "vocabulary          vocabulary          \n",
      "twenty              twenty              \n",
      "words               word                \n",
      "would               would               \n",
      "fit                 fit                 \n",
      "computer            computer            \n",
      "memory              memory              \n",
      "time91493           time91493           \n",
      "1970s               1970s               \n",
      "1970s               1970s               \n",
      "many                many                \n",
      "programmers         programmers         \n",
      "began               begin               \n",
      "write               write               \n",
      "conceptual          conceptual          \n",
      "ontologies          ontologies          \n",
      "structured          structure           \n",
      "realworld           realworld           \n",
      "information         information         \n",
      "computerunderstandablecomputerunderstandable\n",
      "data                data                \n",
      "examples            examples            \n",
      "margie              margie              \n",
      "schank              schank              \n",
      "1975                1975                \n",
      "sam                 sam                 \n",
      "cullingford         cullingford         \n",
      "1978                1978                \n",
      "pam                 pam                 \n",
      "wilensky            wilensky            \n",
      "1978                1978                \n",
      "talespin            talespin            \n",
      "meehan              meehan              \n",
      "1976                1976                \n",
      "qualm               qualm               \n",
      "lehnert             lehnert             \n",
      "1977                1977                \n",
      "politics            politics            \n",
      "carbonell           carbonell           \n",
      "1979                1979                \n",
      "plot                plot                \n",
      "units               units               \n",
      "lehnert             lehnert             \n",
      "1981                1981                \n",
      "time                time                \n",
      "first               first               \n",
      "chatterbots         chatterbots         \n",
      "written             write               \n",
      "eg                  eg                  \n",
      "parry               parry               \n",
      "1980s               1980s               \n",
      "1980s               1980s               \n",
      "early               early               \n",
      "1990s               1990s               \n",
      "mark                mark                \n",
      "heyday              heyday              \n",
      "symbolic            symbolic            \n",
      "methods             methods             \n",
      "nlp                 nlp                 \n",
      "focus               focus               \n",
      "areas               areas               \n",
      "time                time                \n",
      "included            include             \n",
      "research            research            \n",
      "rulebased           rulebased           \n",
      "parsing             parse               \n",
      "eg                  eg                  \n",
      "development         development         \n",
      "hpsg                hpsg                \n",
      "computational       computational       \n",
      "operationalization  operationalization  \n",
      "generative          generative          \n",
      "grammar             grammar             \n",
      "morphology          morphology          \n",
      "eg                  eg                  \n",
      "twolevel            twolevel            \n",
      "morphology91593     morphology91593     \n",
      "semantics           semantics           \n",
      "eg                  eg                  \n",
      "lesk                lesk                \n",
      "algorithm           algorithm           \n",
      "reference           reference           \n",
      "eg                  eg                  \n",
      "within              within              \n",
      "centering           center              \n",
      "theory91693         theory91693         \n",
      "areas               areas               \n",
      "natural             natural             \n",
      "language            language            \n",
      "understanding       understand          \n",
      "eg                  eg                  \n",
      "rhetorical          rhetorical          \n",
      "structure           structure           \n",
      "theory              theory              \n",
      "lines               line                \n",
      "research            research            \n",
      "continued           continue            \n",
      "eg                  eg                  \n",
      "development         development         \n",
      "chatterbots         chatterbots         \n",
      "racter              racter              \n",
      "jabberwacky         jabberwacky         \n",
      "important           important           \n",
      "development         development         \n",
      "eventually          eventually          \n",
      "led                 lead                \n",
      "statistical         statistical         \n",
      "turn                turn                \n",
      "1990s               1990s               \n",
      "rising              rise                \n",
      "importance          importance          \n",
      "quantitative        quantitative        \n",
      "evaluation          evaluation          \n",
      "period91793         period91793         \n",
      "statistical         statistical         \n",
      "nlp                 nlp                 \n",
      "1990s–2010sedit     1990s–2010sedit     \n",
      "1980s               1980s               \n",
      "natural             natural             \n",
      "language            language            \n",
      "processing          process             \n",
      "systems             systems             \n",
      "based               base                \n",
      "complex             complex             \n",
      "sets                set                 \n",
      "handwritten         handwritten         \n",
      "rules               rule                \n",
      "starting            start               \n",
      "late                late                \n",
      "1980s               1980s               \n",
      "however             however             \n",
      "revolution          revolution          \n",
      "natural             natural             \n",
      "language            language            \n",
      "processing          process             \n",
      "introduction        introduction        \n",
      "machine             machine             \n",
      "learning            learn               \n",
      "algorithms          algorithms          \n",
      "language            language            \n",
      "processing          process             \n",
      "due                 due                 \n",
      "steady              steady              \n",
      "increase            increase            \n",
      "computational       computational       \n",
      "power               power               \n",
      "see                 see                 \n",
      "moores              moor                \n",
      "law                 law                 \n",
      "gradual             gradual             \n",
      "lessening           lessen              \n",
      "dominance           dominance           \n",
      "chomskyan           chomskyan           \n",
      "theories            theories            \n",
      "linguistics         linguistics         \n",
      "eg                  eg                  \n",
      "transformational    transformational    \n",
      "grammar             grammar             \n",
      "whose               whose               \n",
      "theoretical         theoretical         \n",
      "underpinnings       underpinnings       \n",
      "discouraged         discourage          \n",
      "sort                sort                \n",
      "corpus              corpus              \n",
      "linguistics         linguistics         \n",
      "underlies           underlie            \n",
      "machinelearning     machinelearning     \n",
      "approach            approach            \n",
      "language            language            \n",
      "processing91893     processing91893     \n",
      "1990s               1990s               \n",
      "many                many                \n",
      "notable             notable             \n",
      "early               early               \n",
      "successes           successes           \n",
      "statistical         statistical         \n",
      "methods             methods             \n",
      "nlp                 nlp                 \n",
      "occurred            occur               \n",
      "field               field               \n",
      "machine             machine             \n",
      "translation         translation         \n",
      "due                 due                 \n",
      "especially          especially          \n",
      "work                work                \n",
      "ibm                 ibm                 \n",
      "research            research            \n",
      "ibm                 ibm                 \n",
      "alignment           alignment           \n",
      "models              model               \n",
      "systems             systems             \n",
      "able                able                \n",
      "take                take                \n",
      "advantage           advantage           \n",
      "existing            exist               \n",
      "multilingual        multilingual        \n",
      "textual             textual             \n",
      "corpora             corpora             \n",
      "produced            produce             \n",
      "parliament          parliament          \n",
      "canada              canada              \n",
      "european            european            \n",
      "union               union               \n",
      "result              result              \n",
      "laws                laws                \n",
      "calling             call                \n",
      "translation         translation         \n",
      "governmental        governmental        \n",
      "proceedings         proceed             \n",
      "official            official            \n",
      "languages           languages           \n",
      "corresponding       correspond          \n",
      "systems             systems             \n",
      "government          government          \n",
      "however             however             \n",
      "systems             systems             \n",
      "depended            depend              \n",
      "corpora             corpora             \n",
      "specifically        specifically        \n",
      "developed           develop             \n",
      "tasks               task                \n",
      "implemented         implement           \n",
      "systems             systems             \n",
      "often               often               \n",
      "continues           continue            \n",
      "major               major               \n",
      "limitation          limitation          \n",
      "success             success             \n",
      "systems             systems             \n",
      "result              result              \n",
      "great               great               \n",
      "deal                deal                \n",
      "research            research            \n",
      "gone                go                  \n",
      "methods             methods             \n",
      "effectively         effectively         \n",
      "learning            learn               \n",
      "limited             limit               \n",
      "amounts             amount              \n",
      "data                data                \n",
      "2000s               2000s               \n",
      "growth              growth              \n",
      "web                 web                 \n",
      "increasing          increase            \n",
      "amounts             amount              \n",
      "raw                 raw                 \n",
      "unannotated         unannotated         \n",
      "language            language            \n",
      "data                data                \n",
      "become              become              \n",
      "available           available           \n",
      "since               since               \n",
      "mid1990s            mid1990s            \n",
      "research            research            \n",
      "thus                thus                \n",
      "increasingly        increasingly        \n",
      "focused             focus               \n",
      "unsupervised        unsupervised        \n",
      "semisupervised      semisupervised      \n",
      "learning            learn               \n",
      "algorithms          algorithms          \n",
      "algorithms          algorithms          \n",
      "learn               learn               \n",
      "data                data                \n",
      "handannotated       handannotated       \n",
      "desired             desire              \n",
      "answers             answer              \n",
      "using               use                 \n",
      "combination         combination         \n",
      "annotated           annotate            \n",
      "nonannotated        nonannotated        \n",
      "data                data                \n",
      "generally           generally           \n",
      "task                task                \n",
      "much                much                \n",
      "difficult           difficult           \n",
      "supervised          supervise           \n",
      "learning            learn               \n",
      "typically           typically           \n",
      "produces            produce             \n",
      "less                less                \n",
      "accurate            accurate            \n",
      "results             result              \n",
      "given               give                \n",
      "amount              amount              \n",
      "input               input               \n",
      "data                data                \n",
      "however             however             \n",
      "enormous            enormous            \n",
      "amount              amount              \n",
      "nonannotated        nonannotated        \n",
      "data                data                \n",
      "available           available           \n",
      "including           include             \n",
      "among               among               \n",
      "things              things              \n",
      "entire              entire              \n",
      "content             content             \n",
      "world               world               \n",
      "wide                wide                \n",
      "web                 web                 \n",
      "often               often               \n",
      "make                make                \n",
      "inferior            inferior            \n",
      "results             result              \n",
      "algorithm           algorithm           \n",
      "used                use                 \n",
      "low                 low                 \n",
      "enough              enough              \n",
      "time                time                \n",
      "complexity          complexity          \n",
      "practical           practical           \n",
      "neural              neural              \n",
      "nlp                 nlp                 \n",
      "presentedit         presentedit         \n",
      "2003                2003                \n",
      "word                word                \n",
      "ngram               ngram               \n",
      "model               model               \n",
      "time                time                \n",
      "best                best                \n",
      "statistical         statistical         \n",
      "algorithm           algorithm           \n",
      "outperformed        outperform          \n",
      "multilayer          multilayer          \n",
      "perceptron          perceptron          \n",
      "single              single              \n",
      "hidden              hide                \n",
      "layer               layer               \n",
      "context             context             \n",
      "length              length              \n",
      "several             several             \n",
      "words               word                \n",
      "trained             train               \n",
      "14                  14                  \n",
      "million             million             \n",
      "words               word                \n",
      "cpu                 cpu                 \n",
      "cluster             cluster             \n",
      "language            language            \n",
      "modelling           model               \n",
      "yoshua              yoshua              \n",
      "bengio              bengio              \n",
      "coauthors91993      coauthors91993      \n",
      "2010                2010                \n",
      "tomáš               tomáš               \n",
      "mikolov             mikolov             \n",
      "phd                 phd                 \n",
      "student             student             \n",
      "brno                brno                \n",
      "university          university          \n",
      "technology          technology          \n",
      "coauthors           coauthors           \n",
      "applied             apply               \n",
      "simple              simple              \n",
      "recurrent           recurrent           \n",
      "neural              neural              \n",
      "network             network             \n",
      "single              single              \n",
      "hidden              hide                \n",
      "layer               layer               \n",
      "language            language            \n",
      "modelling911093     modelling911093     \n",
      "following           follow              \n",
      "years               years               \n",
      "went                go                  \n",
      "develop             develop             \n",
      "word2vec            word2vec            \n",
      "2010s               2010s               \n",
      "representation      representation      \n",
      "learning            learn               \n",
      "deep                deep                \n",
      "neural              neural              \n",
      "networkstyle        networkstyle        \n",
      "featuring           feature             \n",
      "many                many                \n",
      "hidden              hide                \n",
      "layers              layer               \n",
      "machine             machine             \n",
      "learning            learn               \n",
      "methods             methods             \n",
      "became              become              \n",
      "widespread          widespread          \n",
      "natural             natural             \n",
      "language            language            \n",
      "processing          process             \n",
      "popularity          popularity          \n",
      "due                 due                 \n",
      "partly              partly              \n",
      "flurry              flurry              \n",
      "results             result              \n",
      "showing             show                \n",
      "techniques911193911293techniques911193911293\n",
      "achieve             achieve             \n",
      "stateoftheart       stateoftheart       \n",
      "results             result              \n",
      "many                many                \n",
      "natural             natural             \n",
      "language            language            \n",
      "tasks               task                \n",
      "eg                  eg                  \n",
      "language            language            \n",
      "modeling911393      modeling911393      \n",
      "parsing911493911593 parsing911493911593 \n",
      "increasingly        increasingly        \n",
      "important           important           \n",
      "medicine            medicine            \n",
      "healthcare          healthcare          \n",
      "nlp                 nlp                 \n",
      "helps               help                \n",
      "analyze             analyze             \n",
      "notes               note                \n",
      "text                text                \n",
      "electronic          electronic          \n",
      "health              health              \n",
      "records             record              \n",
      "would               would               \n",
      "otherwise           otherwise           \n",
      "inaccessible        inaccessible        \n",
      "study               study               \n",
      "seeking             seek                \n",
      "improve             improve             \n",
      "care911693          care911693          \n",
      "protect             protect             \n",
      "patient             patient             \n",
      "privacy911793       privacy911793       \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classmwheading mwheading3symbolic nap 1950s – early 1990sedit premise symbolic nap wellsummarized john series chinese room experiment given collection rules eg chinese phrasebook questions watching answers computer emulated natural language understanding nap tasks applying rules data confront 1950s georgetown experiment 1954 involved fully automatic translation sixty russian sentences english authors claimed within three five years machine translation would solved problem91293 however real progress much slower iliac report 1966 found ten years research failed fulfill expectations funding machine translation dramatically reduced little research machine translation conducted america though research continued elsewhere japan europe91393 late 1980s first statistical machine translation systems developed 1960s notably successful natural language processing systems developed 1960s shrill natural language system working restricted blocks worlds restricted vocabularies eliza stimulation rogerian psychotherapist written joseph weizenbaum 1964 1966 using almost information human thought emotion eliza sometimes provided startling humanlike interaction patient exceeded small knowledge base eliza might provide genetic response example responding head hurts say head hurts ross quillians successful work natural language demonstrated vocabulary twenty words would fit computer memory time91493 1970s 1970s many programme began write conceptual ontologies structures realworld information computerunderstandable data examples margin thank 1975 sam cullingford 1978 am wilensky 1978 talespin mean 1976 qualms lembert 1977 politics cardinell 1979 plot units lembert 1981 time first chatterbots written eg parry 1980s 1980s early 1990s mark headway symbolic methods nap focus areas time included research released passing eg development his computational operationalization generative grammar morphology eg twolevel morphology91593 semantic eg less algorithm reference eg within entering theory91693 areas natural language understanding eg rhetoric structure theory lines research continued eg development chatterbots after jabberwacky important development eventually led statistical turn 1990s rising importance quantitative evaluation period91793 statistical nap 1990s–2010sedit 1980s natural language processing systems based complex sets handwritten rules starting late 1980s however revolution natural language processing introduction machine learning algorithms language processing due steady increase computational power see moore law gradual lessening dominance chomskyan theories linguistic eg transformation grammar whose theoretical underpinnings discouraged sort corpus linguistic undergoes machinelearning approach language processing91893 1990s many notable early successes statistical methods nap occurred field machine translation due especially work ism research ism ligament models systems able take advantage existing multilingual texture corpora produced parliament canada european union result laws calling translation governmental proceedings official languages corresponding systems government however systems depended corpora specifically developed tasks implements systems often continues major limitation success systems result great deal research gone methods effectively learning limited amounts data 2000s growth web increasing amounts raw unannotated language data become available since mid1990s research thus increasingly focused supervised semisupervised learning algorithms algorithms learn data handannotated desired answers using combination annotated nonannotated data generally task much difficult supervised learning typically produces less accurate results given amount input data however enormous amount nonannotated data available including among things entire content world wide web often make inferior results algorithm used low enough time complexity practical neutral nap presented 2003 word gram model time best statistical algorithm outperformed multilayer perception single hidden layer context length several words trained 14 million words cup cluster language modeling joshua begin coauthors91993 2010 tom sokolov pad student no university technology authors applied simple recurrent neutral network single hidden layer language modelling911093 following years went develop word2vec 2010s representation learning deep neutral networkstyle fearing many hidden layers machine learning methods became widespread natural language processing popularity due partly flurry results showing techniques911193911293 achieve stateoftheart results many natural language tasks eg language modeling911393 parsing911493911593 increasingly important medicine healthcare nap helps analyze notes text electronic health records would otherwise inaccessible study seeking improve care911693 protect patient privacy911793\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "textBlb = TextBlob(df)\n",
    "\n",
    "corrected_strng=textBlb.correct().string\n",
    "print(corrected_strng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classmwheading', 'mwheading3symbolic', 'nap', '1950s', '–', 'early', '1990sedit', 'premise', 'symbolic', 'nap', 'wellsummarized', 'john', 'series', 'chinese', 'room', 'experiment', 'given', 'collection', 'rules', 'eg', 'chinese', 'phrasebook', 'questions', 'watching', 'answers', 'computer', 'emulated', 'natural', 'language', 'understanding', 'nap', 'tasks', 'applying', 'rules', 'data', 'confront', '1950s', 'georgetown', 'experiment', '1954', 'involved', 'fully', 'automatic', 'translation', 'sixty', 'russian', 'sentences', 'english', 'authors', 'claimed', 'within', 'three', 'five', 'years', 'machine', 'translation', 'would', 'solved', 'problem91293', 'however', 'real', 'progress', 'much', 'slower', 'iliac', 'report', '1966', 'found', 'ten', 'years', 'research', 'failed', 'fulfill', 'expectations', 'funding', 'machine', 'translation', 'dramatically', 'reduced', 'little', 'research', 'machine', 'translation', 'conducted', 'america', 'though', 'research', 'continued', 'elsewhere', 'japan', 'europe91393', 'late', '1980s', 'first', 'statistical', 'machine', 'translation', 'systems', 'developed', '1960s', 'notably', 'successful', 'natural', 'language', 'processing', 'systems', 'developed', '1960s', 'shrill', 'natural', 'language', 'system', 'working', 'restricted', 'blocks', 'worlds', 'restricted', 'vocabularies', 'eliza', 'stimulation', 'rogerian', 'psychotherapist', 'written', 'joseph', 'weizenbaum', '1964', '1966', 'using', 'almost', 'information', 'human', 'thought', 'emotion', 'eliza', 'sometimes', 'provided', 'startling', 'humanlike', 'interaction', 'patient', 'exceeded', 'small', 'knowledge', 'base', 'eliza', 'might', 'provide', 'genetic', 'response', 'example', 'responding', 'head', 'hurts', 'say', 'head', 'hurts', 'ross', 'quillians', 'successful', 'work', 'natural', 'language', 'demonstrated', 'vocabulary', 'twenty', 'words', 'would', 'fit', 'computer', 'memory', 'time91493', '1970s', '1970s', 'many', 'programme', 'began', 'write', 'conceptual', 'ontologies', 'structures', 'realworld', 'information', 'computerunderstandable', 'data', 'examples', 'margin', 'thank', '1975', 'sam', 'cullingford', '1978', 'am', 'wilensky', '1978', 'talespin', 'mean', '1976', 'qualms', 'lembert', '1977', 'politics', 'cardinell', '1979', 'plot', 'units', 'lembert', '1981', 'time', 'first', 'chatterbots', 'written', 'eg', 'parry', '1980s', '1980s', 'early', '1990s', 'mark', 'headway', 'symbolic', 'methods', 'nap', 'focus', 'areas', 'time', 'included', 'research', 'released', 'passing', 'eg', 'development', 'his', 'computational', 'operationalization', 'generative', 'grammar', 'morphology', 'eg', 'twolevel', 'morphology91593', 'semantic', 'eg', 'less', 'algorithm', 'reference', 'eg', 'within', 'entering', 'theory91693', 'areas', 'natural', 'language', 'understanding', 'eg', 'rhetoric', 'structure', 'theory', 'lines', 'research', 'continued', 'eg', 'development', 'chatterbots', 'after', 'jabberwacky', 'important', 'development', 'eventually', 'led', 'statistical', 'turn', '1990s', 'rising', 'importance', 'quantitative', 'evaluation', 'period91793', 'statistical', 'nap', '1990s–2010sedit', '1980s', 'natural', 'language', 'processing', 'systems', 'based', 'complex', 'sets', 'handwritten', 'rules', 'starting', 'late', '1980s', 'however', 'revolution', 'natural', 'language', 'processing', 'introduction', 'machine', 'learning', 'algorithms', 'language', 'processing', 'due', 'steady', 'increase', 'computational', 'power', 'see', 'moore', 'law', 'gradual', 'lessening', 'dominance', 'chomskyan', 'theories', 'linguistic', 'eg', 'transformation', 'grammar', 'whose', 'theoretical', 'underpinnings', 'discouraged', 'sort', 'corpus', 'linguistic', 'undergoes', 'machinelearning', 'approach', 'language', 'processing91893', '1990s', 'many', 'notable', 'early', 'successes', 'statistical', 'methods', 'nap', 'occurred', 'field', 'machine', 'translation', 'due', 'especially', 'work', 'ism', 'research', 'ism', 'ligament', 'models', 'systems', 'able', 'take', 'advantage', 'existing', 'multilingual', 'texture', 'corpora', 'produced', 'parliament', 'canada', 'european', 'union', 'result', 'laws', 'calling', 'translation', 'governmental', 'proceedings', 'official', 'languages', 'corresponding', 'systems', 'government', 'however', 'systems', 'depended', 'corpora', 'specifically', 'developed', 'tasks', 'implements', 'systems', 'often', 'continues', 'major', 'limitation', 'success', 'systems', 'result', 'great', 'deal', 'research', 'gone', 'methods', 'effectively', 'learning', 'limited', 'amounts', 'data', '2000s', 'growth', 'web', 'increasing', 'amounts', 'raw', 'unannotated', 'language', 'data', 'become', 'available', 'since', 'mid1990s', 'research', 'thus', 'increasingly', 'focused', 'supervised', 'semisupervised', 'learning', 'algorithms', 'algorithms', 'learn', 'data', 'handannotated', 'desired', 'answers', 'using', 'combination', 'annotated', 'nonannotated', 'data', 'generally', 'task', 'much', 'difficult', 'supervised', 'learning', 'typically', 'produces', 'less', 'accurate', 'results', 'given', 'amount', 'input', 'data', 'however', 'enormous', 'amount', 'nonannotated', 'data', 'available', 'including', 'among', 'things', 'entire', 'content', 'world', 'wide', 'web', 'often', 'make', 'inferior', 'results', 'algorithm', 'used', 'low', 'enough', 'time', 'complexity', 'practical', 'neutral', 'nap', 'presented', '2003', 'word', 'gram', 'model', 'time', 'best', 'statistical', 'algorithm', 'outperformed', 'multilayer', 'perception', 'single', 'hidden', 'layer', 'context', 'length', 'several', 'words', 'trained', '14', 'million', 'words', 'cup', 'cluster', 'language', 'modeling', 'joshua', 'begin', 'coauthors91993', '2010', 'tom', 'sokolov', 'pad', 'student', 'no', 'university', 'technology', 'authors', 'applied', 'simple', 'recurrent', 'neutral', 'network', 'single', 'hidden', 'layer', 'language', 'modelling911093', 'following', 'years', 'went', 'develop', 'word2vec', '2010s', 'representation', 'learning', 'deep', 'neutral', 'networkstyle', 'fearing', 'many', 'hidden', 'layers', 'machine', 'learning', 'methods', 'became', 'widespread', 'natural', 'language', 'processing', 'popularity', 'due', 'partly', 'flurry', 'results', 'showing', 'techniques911193911293', 'achieve', 'stateoftheart', 'results', 'many', 'natural', 'language', 'tasks', 'eg', 'language', 'modeling911393', 'parsing911493911593', 'increasingly', 'important', 'medicine', 'healthcare', 'nap', 'helps', 'analyze', 'notes', 'text', 'electronic', 'health', 'records', 'would', 'otherwise', 'inaccessible', 'study', 'seeking', 'improve', 'care911693', 'protect', 'patient', 'privacy911793']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = corrected_strng\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
